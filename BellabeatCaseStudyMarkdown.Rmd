---
title: "**Bellabeat Case Study**"
author: "Phua Jun Yu"
date: "2023-07-24"
output: github_document
indent: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(data.table)
library(ggplot2)
library(gridExtra)
```


> This is an R markdown document to briefly showcase my work in Bellabeat data analysis case study as a capstone project for Google Data Analytics Certificate specialization on Coursera.

# **Business Task**
### **Stakeholders**
> Primary Stakeholder -- Sršen (Co-founder, chief creative officer) <br/> Secondary Stakeholder -- Sando Mur(Executive team)


### **Description of business task  **  
> Analyze smart device usage data in order to gain insight into how consumers use non-Bellabeat smart devices and apply these insights to any one of the Bellabeat products on marketing strategy.


### **Project target**
> 1.&nbsp; Identify at least 3 trend/relationships from data.
<br/>2.&nbsp; Identify underlying consumers' behaviour pattern description for each of the trend in target 1.
<br/>3.&nbsp; Come up with at least 1 aspect of suggested marketing strategy of Bellabeat.


### **Important Info about Bellabeat**
> 1.Bellabeat is still a small company, with potential to grow large.<br/>
2.There are 5 Bellabeat products <br/>
  >> Bellabeat app<br/>
  >> Leaf tracker : tracker as bracelet, necklace, or clip.<br/>
  >> Time : tracker as a watch.<br/>
  >> Spring : water bottle<br/>
  >> Bellabeat membership : subscription for 24/7 personalized health guidance<br/>
3.Main customer group of Bellabeat are women around the globe.<br/>
4.Bellabeat sell through a number of online retailers as well as their own e-commerce module on their website.<br/>
5.Bellabeat's marketing strategy currently focuses more on digital marketing, such as posts on social media and paid ads on YouTube and Google.<br/>
6.Mainly collected data : activity, sleep, and stress.<br/>

***

# **Data preparation**


### **Data source**
> FitBit Fitness Tracker Data https://www.kaggle.com/datasets/arashnic/fitbit


### **Limitation of the data**
> 1.Sample size : The data consist of data from 30 Fitbit users, which holds a minimum statistical significane.<br/>
2.Time period : The data is about the time period between 03.12.2016-05.12.2016, users' behaviour might have changed.<br/>
3.Third party data : This is a public data on Kaggle, generated by respondents to a distributed survey via Amazon Mechanical Turk.<br/>
4.Lack of demographic information : The data does not contain any direct information about the user profile.<br/>
5.Lack of usage information : 


# **Data Inspection & Data Processing **
> The data consist of 18 CSV files, of which some of them have overlapping data. There are 12 of them that I choose to use in analysis.


### **Description of data**
> [Click here](https://www.fitabase.com/media/1930/fitabasedatadictionary102320.pdf) to get Metadata documentation of the data.


### **Data problems check**

> Check for missing values
>
>>Only data about weight log "weightLogInfo_merged.csv" has empty/missing values, mostly in 'fat' column.
>
>> <details>
>>   <summary font size=10>R code to check for missing values</summary>
>> ``` {r }
>> dailyActivity_raw = read.csv(r'[D:\School\Reading\Google Data Analytic Cert\Case Study\Data\Fitabase Data 4.12.16-5.12.16\dailyActivity_merged.csv]')
>> fileNames <- c('dailyActivity_merged.csv',
>>              'heartrate_seconds_merged.csv',
>>              'minuteCaloriesNarrow_merged.csv',
>>              'minuteCaloriesWide_merged.csv',
>>              'minuteIntensitiesNarrow_merged.csv',
>>              'minuteIntensitiesWide_merged.csv',
>>              'minuteMETsNarrow_merged.csv',
>>              'minuteSleep_merged.csv',
>>              'minuteStepsNarrow_merged.csv',
>>              'minuteStepsWide_merged.csv',
>>              'sleepDay_merged.csv',
>>              'weightLogInfo_merged.csv'
>>              )
>> fileBaseDir <- r'[D:\School\Reading\Google Data Analytic Cert\Case Study\Data\Fitabase Data 4.12.16-5.12.16\]'
>> # checking for empty/missing values using is.na function
>> for (fileName in fileNames){
>>  file <- ""
>>  file <- paste(fileBaseDir ,fileName,sep = "")
>>  data_raw <- read.csv(file)
>>  noNa <- which(is.na(data_raw))
>>  print(paste("The file ",fileName," contains ",length(noNa)," empty values"))
>> }
>> ```
>> </details> 
>
>> | File Name   | Done | Method/tool used  | Result & remark  | 
>> | :-------------- | :---- | :------------- | :---------------------------------- |
>> dailyActivity_merged.csv|T|Excel filter function|No missing values found.
>> heartrate_seconds_merged.csv|T|R isna() function|No missing values found.
>> minuteCaloriesNarrow_merged.csv|T|R isna() function|No missing values found.
>> minuteCaloriesWide_merged.csv|T|R isna() function|No missing values found.
>> minuteIntensitiesNarrow_merged.csv|T|R isna() function|No missing values found.
>> minuteIntensitiesWide_merged.csv|T|R isna() function|No missing values found.
>> minuteMETsNarrow_merged.csv|T|R isna() function|No missing values found.
>> minuteSleep_merged.csv|T|R isna() function|No missing values found.
>> minuteStepsNarrow_merged.csv|T|R isna() function|No missing values found.
>> minuteStepsWide_merged.csv|T|R isna() function|No missing values found.
>> sleepDay_merged.csv|T|R isna() function|No missing values found.
>> weightLogInfo_merged.csv|T|R isna() function|65 Missing values found. All of the missing values are in the ‘fat’ column.


> Detect and remove duplicates
>



> Formatting check
>
>> | File | Done | Remark | 
>> | :------------- | :---- | :------------------------------ |
>> dailyActivity_merged.csv|T|"Ambiguous date format : Presentation of “ActivityDate” column in Excel varies because some for dates whose day is less than 13, Excel can not determine without ambiguity the format of the date."
>> heartrate_seconds_merged.csv|T|Ambiguous date format :  “Time” column
>> minuteCaloriesNarrow_merged.csv|T|Ambiguous date format :  “ActivityColumn” column the format of the date.
>> minuteCaloriesWide_merged.csv|T|Ambiguous date format :  ActivityHour” column.
>> minuteIntensitiesNarrow_merged.csv|T|Ambiguous date format :  “ActivityMinutes” column
>> minuteIntensitiesWide_merged.csv|T|Ambiguous date format :  “ActivityHour” column
>> minuteMETsNarrow_merged.csv|T|Ambiguous date format : “ActivityHour” column
>> minuteSleep_merged.csv|T|Ambiguous date format : “date” column
>> minuteStepsNarrow_merged.csv|T|Ambiguous date format : “ActivityMinute” column
>> minuteStepsWide_merged.csv|T|Ambiguous date format : “ActivityHour” column
>> sleepDay_merged.csv|T|Ambiguous date format : “SleepDay” column
>> weightLogInfo_merged.csv|T|Ambiguous date format : “Date” column
>
>> <details>
>>   <summary>Check if numeric values are consistent</summary>
>> ``` {r }
>> # check for format consistency and data type
>> file <- 'dailyActivity_merged.csv'
>> data <- read.csv(paste(fileBaseDir,file,sep=""))
>> colnames(data)
>> 
>> # for dailyActivity, check if the columns are all numeric 
>> col_numeric <- c('TotalSteps','TotalDistance','TrackerDistance','LoggedActivitiesDistance',
>>             'VeryActiveDistance','ModeratelyActiveDistance','LightActiveDistance',
>>             'SedentaryActiveDistance','VeryActiveMinutes','FairlyActiveMinutes',
>>             'LightlyActiveMinutes','SedentaryMinutes','Calories')
>> #check_numeric(dataDf=data,colNum=col_numeric)
>> 
>> 
>> file <- 'heartrate_seconds_merged.csv'
>> data <- read.csv(paste(fileBaseDir,file,sep=""))
>> colnames(data)
>> col_numeric <- c('Value')
>> #check_numeric(dataDf=data,colNum=col_numeric)
>> 
>> file <- 'minuteCaloriesWide_merged.csv'
>> data <- read.csv(paste(fileBaseDir,file,sep=""))
>> colnames(data)
>> col_numeric <- c('Value')
>> #check_numeric(dataDf=data,colNum=col_numeric)
>> 
>> ```
>> </details> 


> Outliers
>
>> Found potential outliers in almost all files.
>
>> Found that some data record contains unreasonable amount of recorded activity minute. This indicates that some data record might not reflect the user's activity on that day.Now we combine with sleep data to see if it is just that the person was sleeping all day (weird but not impossible),it might or might not because the person has a long sleeping time on that day, we have to check the sum of activity time and time in bed to see
>
>> <details>
>>  <summary>Check outliers for numeric columns</summary>
>> ``` {r }
>> col_to_skip <- c('Id',
>>                 'ActivityDate',
>>                 'Time',
>>                 'ActivityHour',
>>                 'ActivityMinute',
>>                 'date',
>>                 'logId',
>>                 'SleepDay',
>>                 'IsManualReport')
>> i <- 0
>>
>> fileNames <- c('dailyActivity_merged.csv',
>>               'heartrate_seconds_merged.csv',
>>               'minuteCaloriesNarrow_merged.csv',
>>               'minuteCaloriesWide_merged.csv',
>>               'minuteIntensitiesNarrow_merged.csv',
>>               'minuteIntensitiesWide_merged.csv',
>>               'minuteMETsNarrow_merged.csv',
>>               'minuteSleep_merged.csv',
>>               'minuteStepsNarrow_merged.csv',
>>               'minuteStepsWide_merged.csv',
>>               'sleepDay_merged.csv'
>> )
>>
>> # checking for potential outliers
>> for (fileName in fileNames){
>>  file <- ""
>>  file <- paste(fileBaseDir ,fileName,sep = "")
>>  data_raw <- read.csv(file)
>>  cols <- colnames(data_raw)
>>  for(col in cols){
>>    allNumeric <- TRUE
>>    if(col %in% col_to_skip) next
>>    # if it is all numeric(quantitative data)
>>    t <- is.numeric(data_raw[[col]])
>>    t1 <-  is.numeric(data_raw[col])
>>    for(no in data_raw[[col]]){
>>      if(!is.numeric(no)){
>>        allNumeric <- FALSE
>>      }
>>    }
>>
>>    if(allNumeric){
>>      # look for potential outliers
>>      x <- data_raw[[col]]
>>      Qs <- quantile(x, prob=c(.25,.5,.75), type=1)
>>      IQR = Qs['75%'] - Qs['25%']
>>      upper_lim = Qs['75%'] + IQR
>>      lower_lim = Qs['25%'] - IQR
>>      noOutliers <- sum(x<lower_lim | x>upper_lim)
>>      # print(paste("Upperlim: ",upper_lim,", lowerLim: ",lower_lim))
>>      if(noOutliers != 0)print(paste("In file ",fileName," column ",col," contains ",noOutliers," potential outliers"))
>>      else print("no potential outliers")
>>    }else{
>>      print("not all numeric")
>>    }
>>    # else it is qualitative data and we do nothing to them for now
>>  }
>> }
>>
>> check_numeric <- function(dataDf,colNum){
>>  for(col in col_numeric){
>>    allNumeric <- TRUE
>>    allPos <- TRUE
>>    for(no in data[[col]]){
>>      if(!is.numeric(no)){
>>        allNumeric <- FALSE
>>      }
>>      if(sum(data[[col]]<0)>0){
>>        allPos <- FALSE
>>      }
>>    }
>>    if(!allNumeric){
>>      print(paste(col, " - Not Numeric"))
>>    }else{
>>      print(paste(col, " - Numeric"))
>>    }
>>    if(allPos){
>>      print(paste(col, " - All pos"))
>>    }else{
>>      print(paste(col, " - Not all pos"))
>>    }
>>  }
>> }
>>
>> ```
>> </details> 
>
>> File Name | Done | Method/tool used | Remark
>> | :--------- | :---- | :--------- | :------------------------- |
>> dailyActivity_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Columns with potential outliers and the number of potential outliers :      (TotalSteps,32) ,(TotalDistance,35),(TrackerDistance,34),(LoggedActivitiesDistance,32),(VeryActiveDistance,106),(ModeratelyActiveDistance,101),(LightActiveDistance,21 ),(SedentaryActiveDistance,82),(VeryActiveMinutes,98),(FairlyActiveMinutes,93),(LightlyActiveMinutes,23),(SedentaryMinutes,9 ),(Calories,49)
>> heartrate_seconds_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  Value  contains  116658  potential outliers
>> minuteCaloriesNarrow_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  Calories  contains  211126  potential outliers
>> minuteCaloriesWide_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|lots of outliers in every column
>> minuteIntensitiesNarrow_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  Intensity  contains  213478  potential outliers
>> minuteIntensitiesWide_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Lots of outliers in every column
>> minuteMETsNarrow_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  METs  contains  250855  potential outliers
>> minuteSleep_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  value  contains  16041  potential outliers
>> minuteStepsNarrow_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Column  Steps  contains  199910  potential outliers
>> minuteStepsWide_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Lot of outliers in every column
>> sleepDay_merged.csv|T|Compute quantiles, IQR and upper,lower limits.|Columns with potential outliers and the number of potential outliers :(TotalSleepRecords,46),(TotalMinutesAsleep,40),(TotalMinutesAsleep,40)
>> weightLogInfo_merged.csv|T|Inspection using Excel|Most values for column “Fat” are missing, outliers depend on how to fill in those values.A “133.5” in kgs for weight log info, potential outlier or error value."
>
>
>> <details>
>>   <summary>Check for seemingly unreasonable values(outliers)</summary>
>> ``` {r }
>> 
>> showPotentialOutliers <- function(fileName,fileBaseDir){
>>   
>>   file <- paste(fileBaseDir ,fileName,sep = "")
>>   print(paste("File : ",file))
>>   sink(paste(file,"_potentialOutliers.txt"))
>>   data_raw <- read.csv(file)
>>   cols <- colnames(data_raw)
>>   
>>   for(col in cols){
>>     noOutliers <- 0
>>     allNumeric <- TRUE
>>     if(col %in% col_to_skip) next
>>     # if it is all numeric(quantitative data)
>>     for(no in data_raw[[col]]){
>>       if(!is.numeric(no)){
>>         allNumeric <- FALSE
>>         break
>>       }
>>     }
>>     # there will be no outliers in none-numeric values
>>     if(!allNumeric) next
>>     
>>     if(allNumeric){
>>       print(paste("potential outliers in ",col," : "))
>>       # look for potential outliers
>>       x <- data_raw[[col]]
>>       Qs <- quantile(x, prob=c(.25,.5,.75), type=1)
>>       IQR = Qs['75%'] - Qs['25%']
>>       upper_lim = Qs['75%'] + IQR
>>       lower_lim = Qs['25%'] - IQR
>>       cat(paste(col,"\n"))
>>       cat(paste("Upperlim: ",upper_lim,", lowerLim: ",lower_lim,"\n"))
>>       noOutliers = sum(x<lower_lim) + sum(x>upper_lim)
>>       for(val in x){
>>         if(val<lower_lim || val>upper_lim){
>>           cat(paste(" - ",val,"\n"))
>>         }
>>       }
>>       
>>       if(noOutliers != 0)print(paste("column ",col," contains >> ",noOutliers," potential outliers"))
>>       else print("no potential outliers")
>>     }
>>   }
>>   sink()
>> }
>> 
>> # judging potential outliers
>> fileNames <- c('dailyActivity_merged.csv',
>>                'heartrate_seconds_merged.csv',
>>                'minuteCaloriesNarrow_merged.csv',
>>                'minuteCaloriesWide_merged.csv',
>>                'minuteIntensitiesNarrow_merged.csv',
>>                'minuteIntensitiesWide_merged.csv',
>>                'minuteMETsNarrow_merged.csv',
>>                'minuteSleep_merged.csv',
>>                'minuteStepsNarrow_merged.csv',
>>                'minuteStepsWide_merged.csv',
>>                'sleepDay_merged.csv'
>> )
>> for(fileName in fileNames){
>>   showPotentialOutliers(fileName,fileBaseDir)
>> }
>> 
>> 
>> 
>> # now investigate outliers that seem unreasonable
>> # sedentaryMinute in dailyActivity_merged.csv 
>> dailyActivity_raw = read.csv(r'[D:\School\Reading\Google Data Analytic Cert\Case Study\Data\Fitabase Data 4.12.16-5.12.16\dailyActivity_merged.csv]')
>> glimpse(dailyActivity_raw)
>> colnames(dailyActivity_raw)
>> dailyActivity_raw[['SedentaryMinutes']]
>> mean(dailyActivity_raw[['SedentaryMinutes']])
>> dailyActivity_raw[['ActivityDate']]
>> # look for sedentary minutes less than 200, which seems unreasonable
>> for(sen in dailyActivity_raw[['SedentaryMinutes']]){
>>   if(sen < 200){
>>     print(sen)
>>   }
>> }
>> # check logical consistency of minutes data -- check for total minutes >> recorded
>> totalMinutes <- dailyActivity_raw[['SedentaryMinutes']] +  dailyActivity_raw[['LightlyActiveMinutes']] +  dailyActivity_raw[['FairlyActiveMinutes']]+  dailyActivity_raw[['VeryActiveMinutes']]
>> mean(totalMinutes)
>> # found that some data record contains unreasonable amount of recorded activity minute
>> # this indicates that some data record might not reflect the user's  activity on that day
>> # now we combine with sleep data to see if it is just that the person was >> sleeping all day (weird but not impossible)
>> # it might or might not because the person has a long sleeping time on that day, we have to check the sum of activity time and time in bed to see
>> 
>> # read sleep data
>> sleep_raw <- read.csv(r'[D:\School\Reading\Google Data Analytic Cert\Case Study\Data\Fitabase Data 4.12.16-5.12.16\sleepDay_merged.csv]')
>> 
>> # split & rename column for data merging
>> sleep_sep <- separate(sleep_raw,"SleepDay",into=c("ActivityDate","Time","Clock"),sep= " +")  
>> 
>> # merge activity and sleep data
>> dailyActivity_sleep_merged <- merge(dailyActivity_raw,sleep_sep,by=c("Id","ActivityDate"))
>> 
>> # check if successfully merged
>> head(dailyActivity_sleep_merged)
>> 
>> # compute the total time in record 
>> sumTime <- dailyActivity_sleep_merged[['SedentaryMinutes']] +  dailyActivity_sleep_merged[['LightlyActiveMinutes']] +  dailyActivity_sleep_merged[['FairlyActiveMinutes']]+  dailyActivity_sleep_merged[['VeryActiveMinutes']] +  dailyActivity_sleep_merged[['TotalTimeInBed']]
>> 
>> dailyActivity_sleep_merged <- dailyActivity_sleep_merged %>%
>>   cbind(sumTime)
>> # average of the total time in record 
>> mean(sumTime) 
>> # average is 1430.891 minutes, which is reasonably 23 hours+ 
>> # still, we can see that for some records, clearly a huge part of the days was not recorded
>> # check how many records has time in record less than 1000 minutes
>> sum(sumTime<900)
>> # remove records with time less than 1000 minutes
>> dailyActivity_sleep_merged <- dailyActivity_sleep_merged %>% filter(sumTime>1000)
>> 
>> # now check for unusually low values of calories expenditure
>> sum(dailyActivity_sleep_merged[["Calories"]]<800)
>> # the number of unusually low values of calories expenditure is not found(was removed in previous steps)
>> 
>> colnames(dailyActivity_sleep_merged)
>> mean(dailyActivity_sleep_merged[["TotalTimeInBed"]])
>> 
>> 
>> ```
>> </details> 
>> 

***

#  **Data analysis**

### **Observations(relationship & trends)**
#####  1. Negative correlation between sedentary minute and lightly active minute
```{r echo=FALSE, message=FALSE,results=FALSE}
lm_fit <- lm(SedentaryMinutes ~ LightlyActiveMinutes, data=dailyActivity_sleep_merged)
lm_fit

ggplot(data = dailyActivity_sleep_merged) +
  geom_point(mapping = aes(x=SedentaryMinutes,y=LightlyActiveMinutes),color='red') +
  geom_abline(slope=lm_fit$coefficients[2],
              intercept=lm_fit$coefficients[1],
              color="blue")+
  ggtitle("Sedentary vs Lightly Active Minutes")
```



##### 2. Negative linear correlation between sedentary minute and sleeping time

> To measure sleep quality, a basic assumption is that a person has better sleep quality the sooner he/she falls asleep and the quicker he/she can get out from the bed after waking up. Thus, we define a metric to measure sleep quality = TimeAsleep/(no of sleep record) 

``` {r echo=FALSE, message=FALSE}
# check if there is any relationship between activity and sleep quality
# compute new metric to assess sleep quality


sleepQuality <- dailyActivity_sleep_merged[["TotalMinutesAsleep"]] /dailyActivity_sleep_merged[["TotalTimeInBed"]]


dailyActivity_sleep_merged <- dailyActivity_sleep_merged %>%
  cbind(sleepQuality)

lm_fit1 <- lm(SedentaryMinutes ~ TotalMinutesAsleep, data=dailyActivity_sleep_merged)

ggplot(data = dailyActivity_sleep_merged) +
  geom_point(mapping = aes(y=SedentaryMinutes,x=TotalMinutesAsleep),color='red') +
  geom_abline(slope=lm_fit1$coefficients[2],
              intercept=lm_fit1$coefficients[1],
              color="blue") +
  ggtitle("Sedentary Time vs Total Sleeping Time")


# does not observe apparent relationship between sleep quality and activity level
# but does found some poeple with bad sleeping quality
```

##### 3. Sedentary minute on average higher on weekday than on weekends
```{r echo=FALSE,message=FALSE,warning=FALSE,results=FALSE}
# check for activity & sleep trend in a week
library(lubridate)
# first need to compute records' day in a week
dayOfWeek <- lubridate::wday(mdy(dailyActivity_sleep_merged$ActivityDate),week_start=1)

dailyActivity_sleep_merged <- dailyActivity_sleep_merged %>%
  cbind(dayOfWeek)

# then aggregate records of same day in a week

days <- dailyActivity_sleep_merged %>% group_by(dayOfWeek) %>% summarize(
  avgSedentaryMinutes=mean(SedentaryMinutes),
  avgLightlyActiveMinutes=mean(LightlyActiveMinutes),
  avgFairlyActiveMinutes=mean(FairlyActiveMinutes),
  avgVeryActiveMinutes=mean(VeryActiveMinutes),
  avgTotalMinutesAsleep=mean(TotalMinutesAsleep),
  avgSleepQuality=mean(sleepQuality),
  avgCalories=mean(Calories),
  avgTotalDistance=mean(TotalDistance),
  avgTotalSteps=mean(TotalSteps),
  .groups='drop')

# check for trends
# possible insight : sedentary minute on average is higher on weekday than on weekend


avgSedentaryMinuteWeekday <- mean(days[days$dayOfWeek<6,][["avgSedentaryMinutes"]])
avgSedentaryMinuteWeekEnd <- mean(days[days$dayOfWeek>5,][["avgSedentaryMinutes"]])

avgSedentaryMinuteWeekday
avgSedentaryMinuteWeekEnd

stringDayOfWeek <- c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")
stringDayOfWeek[days[["dayOfWeek"]]]

days <- days %>% cbind(stringDayOfWeek)
colnames(days)

period <- ifelse(days[["dayOfWeek"]]<6,"weekDay", "weekEnd")
period

colnames(days)
days <- days %>% cbind(period)

ggplot(data=days,aes(x=dayOfWeek,y=avgSedentaryMinutes,fill=period)) +
  geom_bar(stat="identity",size=1.5)+
  geom_segment(aes(x=0.5,xend=5.5,y=avgSedentaryMinuteWeekday,yend=avgSedentaryMinuteWeekday),color='red',size=1)+
  geom_segment(aes(x=5.5,xend=7.5,y=avgSedentaryMinuteWeekEnd,yend=avgSedentaryMinuteWeekEnd),color='blue',size=1)+
  scale_x_discrete(limits=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")) +
  ggtitle("Average Sedentary Time Across Week")


```


##### 4. Total distance&steps has a weak decreasing trend across the week from Monday to Sunday ,except for Saturday
``` {r echo=FALSE,warning=FALSE,message=FALSE,results=FALSE}
# possible insight : total distance&steps has a weak decreasing trend across the week from monday to sunday except for saturday
g1 <- ggplot(data=days,aes(x=dayOfWeek,y=avgTotalDistance,fill=period)) +
  geom_bar(stat="identity")+
  scale_x_discrete(limits=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")) + 
  ggtitle("Average Total Distance Travelled in week")

g2 <- ggplot(data=days,aes(x=dayOfWeek,y=avgTotalSteps,fill=period)) +
  geom_bar(stat="identity")+
  scale_x_discrete(limits=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")) + 
  ggtitle("Average Total Steps in week")



grid.arrange(g1,g2,ncol=2)
```

##### 5. Total minutes asleep on average is slightly higher on weekend than on weekday 

```{r echo=FALSE,warning=FALSE,message=FALSE,results=FALSE}
# possible insight : total minutes asleep on average in higher on weekend than on weekday
avgTotalMinutesAsleepWeekDay <- mean(days[days$dayOfWeek<6,][["avgTotalMinutesAsleep"]])
avgTotalMinutesAsleepWeekEnd <- mean(days[days$dayOfWeek>5,][["avgTotalMinutesAsleep"]])

ggplot(data=days,aes(x=dayOfWeek,y=avgTotalMinutesAsleep,fill=period)) +
  geom_bar(stat="identity",size=1.5)+
  geom_segment(aes(x=0.5,xend=5.5,y=avgTotalMinutesAsleepWeekDay,yend=avgTotalMinutesAsleepWeekDay),color='red',size=1)+
  geom_segment(aes(x=5.5,xend=7.5,y=avgTotalMinutesAsleepWeekEnd,yend=avgTotalMinutesAsleepWeekEnd),color='blue',size=1)+
  scale_x_discrete(limits=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")) + 
  ggtitle("Sleeping time across week")




```

***

### **Insights**
>> From observation 5, we can glimpse some demographic information of composition of user profile in the sample from common sense. This sample seem to consist mostly of working adults who work 5 days per week, from Monday to Friday, in a sense that they have more time to sleep on weekends.
>
>> From observation 3, we can deduce that this sample consist of people who mostly works in the office, which accompanies lot of sedentary time.
>
>> Observation 1 states that there is a negative relationship between sedentary minute and lightly active minute. But there was no apparent relationship found among other types of activity time. From this, we can conclude that sedentary and lightly active time are mutually exclusive, i.e. this represents a choice between sedentary and lightly active way of spending some certain time(for instance commuting). 
>
>> There can be a deduction that the negative relationship between sedentary and lightly active time is due to the different activity pattern between weekdays and weekends, thus invalidating the deduction that people have choice between lightly active and sedentary time. However, negative relationship exists even only within weekdays and weekends data points respectively. Thus the mutual exclusiveness still holds. The negative correlation is stronger on weekdays, which further increases the likelihood of choice of commuting methods being the underlying factor of the mutual exclusiveness between sedentary and lightly active minutes.
>> ```{r echo=FALSE, message=FALSE, results=FALSE}
>> 
>> lm_fit_weekday <- lm(SedentaryMinutes ~ LightlyActiveMinutes, data=dailyActivity_sleep_merged[dailyActivity_sleep_merged$dayOfWeek <6,])
>> lm_fit_weekday
>> 
>> lm_fit_weekend <- lm(SedentaryMinutes ~ LightlyActiveMinutes, data=dailyActivity_sleep_merged[dailyActivity_sleep_merged$dayOfWeek >5,])
>> 
>> lm_fit_weekday
>> lm_fit_weekend
>> 
>> gg1 <- ggplot(data = dailyActivity_sleep_merged[dailyActivity_sleep_merged$dayOfWeek <6,]) +
>>   geom_point(mapping = aes(x=SedentaryMinutes,y=LightlyActiveMinutes),color='red') +
>>   geom_abline(slope=lm_fit_weekday$coefficients[2],
>>               intercept=lm_fit_weekday$coefficients[1],
>>               color="blue")+
>>   ggtitle("On Weekdays")
>> 
>> gg2 <- ggplot(data = dailyActivity_sleep_merged[dailyActivity_sleep_merged$dayOfWeek >5,]) +
>>   geom_point(mapping = aes(x=SedentaryMinutes,y=LightlyActiveMinutes),color='red') +
>>   geom_abline(slope=lm_fit_weekend$coefficients[2],
>>               intercept=lm_fit_weekend$coefficients[1],
>>               color="blue")+
>>   ggtitle("On Weekends")
>> 
>> grid.arrange(gg1,gg2,ncol=2)
>> ```
>
>> Observation 4 states that there is a weak decreasing trend in activity level across the week from Monday to Sunday ,except for Saturday. This might indicate a decreasing energy level throughout the week, or decreasing willingness to commute with lightly active way. 
>
>>  Observation 2 states that there is a negative linear correlation between sedentary minute and sleeping time. But this can be due to the fact that Mon-to-Fri working shift the people in the sample are more sedentary and sleep less on weekdays, while more active and sleep more on weekends. Thus the negative relationship is merely a result of Mon-to-Fri working shift.


***

## **Action suggestion**
> Combining above insights, we propose Bellabeat to adopt a marketing strategy that promotes active commuting. As Bellabeat's main customer group are women around the globe, this can mean to promote active commuting methods that are suitable for white-collar women, such as walking, kick scooter or bicycle. More specifically, this can mean to include features about travelling with bicycle.

***

## **Further analyses/improvements**
>> Greater sample size would provide better credibility of conclusions.
>
>> Inclusion of demographic information would lead to marketing strategy better aimed at Bellabeat's target customer group.
>
>> Use data that is closer to current time so that the effectiveness of the insights has not diminished overtime.



